<p align="center">
  <h1 align="center">A Collection of Text-to-Video Generation Studies</h1>

We maintain this repository to summarize papers and resources related to the text-to-video (T2V) generation task. 

In `reference.bib`, we will summarize and update the bibtex references of up-to-date T2V papers, as well as widely used datasets and toolkits.

If you have any suggestions about this repository, please feel free to [start a new issue](https://github.com/synlp/T2V-Review/issues/new) or [pull requests](https://github.com/synlp/T2V-Review/pulls).

# Table of Contents
- [Table of Contents](#table-of-contents)
  - [Products](#products)
    - [2023](#2023)
  - [Papers](#papers)
    - [2023](#2023-1)

## Products
### 2023
Animate Anyone [[website]](https://humanaigc.github.io/animate-anyone/)
Emu [[website]](https://emu-video.metademolab.com/)
Gen-2 [[website]](https://research.runwayml.com/gen2)
Midjourney [[website]](https://www.midjourney.com/)
Morph Studio [[website]](https://www.morphstudio.com/)
Outfit Anyone [[website]](https://humanaigc.github.io/outfit-anyone/)
Pika [[website]](https://pika.art/login) 
PixelDance [[website]](https://makepixelsdance.github.io/)
VideoPoet [[website]](https://sites.research.google/videopoet/)


## Papers
### 2023

[arXiv 2023] AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning [[paper]](https://openreview.net/pdf?id=Fx2SbBgcte) [[project]](https://animatediff.github.io/)

[arXiv 2023] Control-A-Video: Controllable Text-to-Video Generation with Diffusion Models [[paper]](https://arxiv.org/pdf/2305.13840.pdf) [[code]](https://github.com/Weifeng-Chen/control-a-video) [[demo]](https://huggingface.co/spaces/wf-genius/Control-A-Video) [[project]](https://arxiv.org/pdf/2305.13840.pdf)

[arXiv 2023] ControlVideo: Training-free Controllable Text-to-Video Generation [[paper]](https://arxiv.org/pdf/2305.13077.pdf) [[code]](https://github.com/YBYBZhang/ControlVideo)

[arXiv 2023] Style-A-Video: Agile Diffusion for Arbitrary Text-based Video Style Transfer [[paper]](https://arxiv.org/pdf/2305.05464.pdf)

[arXiv 2023] Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets [[paper]](https://arxiv.org/pdf/2311.15127.pdf)

[CVPR 2023] Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models [[paper]](https://arxiv.org/pdf/2304.08818.pdf) [[project]](https://research.nvidia.com/labs/toronto-ai/VideoLDM/) [[reproduced code]](https://github.com/srpkdyy/VideoLDM)

[CVPR 2023] Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators [[paper]](https://openaccess.thecvf.com/content/ICCV2023/papers/Khachatryan_Text2Video-Zero_Text-to-Image_Diffusion_Models_are_Zero-Shot_Video_Generators_ICCV_2023_paper.pdf) [[code]](https://github.com/Picsart-AI-Research/Text2Video-Zero) [[demo]](https://huggingface.co/spaces/PAIR/Text2Video-Zero) [[project]](https://text2video-zero.github.io/) 

